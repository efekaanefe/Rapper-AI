{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNx24WdJ9FldGu5H+fwPhHX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/efekaanefe/Rapper-AI/blob/main/rapper_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NFXpPuOyTlLd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('eminem-lyrics.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcSvEK1yT_0t",
        "outputId": "460050d7-193e-4103-8a62-539eb363d70a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102390"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word level inputs and targets"
      ],
      "metadata": {
        "id": "mJmUpIdEC3m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split()\n",
        "unique_words = sorted(list(set(words)))\n",
        "vocab_size = len(unique_words)\n",
        "print(f'Unique words/vocabulary size: {vocab_size}')\n",
        "\n",
        "# Create mappings from words to indices and vice versa\n",
        "word_to_idx = {word: idx for idx, word in enumerate(unique_words)}\n",
        "idx_to_word = {idx: word for idx, word in enumerate(unique_words)}\n",
        "\n",
        "# Encode the text using word-level vocabulary\n",
        "encoded_text = np.array([word_to_idx[word] for word in words])\n",
        "\n",
        "seq_length = 10  # Adjust sequence length as needed for words\n",
        "num_samples = len(encoded_text) // seq_length\n",
        "\n",
        "input_sequences = []\n",
        "target_sequences = []\n",
        "\n",
        "# The fix: Ensure all sequences are of the same length by padding shorter ones\n",
        "for i in range(num_samples):\n",
        "    start_idx = i * seq_length\n",
        "    end_idx = start_idx + seq_length\n",
        "\n",
        "    input_seq = encoded_text[start_idx:end_idx]\n",
        "    target_seq = encoded_text[start_idx + 1:end_idx + 1]\n",
        "\n",
        "    # Pad if the sequence is shorter than seq_length\n",
        "    if len(input_seq) < seq_length:\n",
        "        input_seq = np.pad(input_seq, (0, seq_length - len(input_seq)), 'constant', constant_values=0)  # Pad with zeros\n",
        "    if len(target_seq) < seq_length:\n",
        "        target_seq = np.pad(target_seq, (0, seq_length - len(target_seq)), 'constant', constant_values=0)  # Pad with zeros\n",
        "\n",
        "    input_sequences.append(input_seq)\n",
        "    target_sequences.append(target_seq)\n",
        "\n",
        "input_sequences = torch.tensor(np.array(input_sequences), dtype=torch.long)\n",
        "target_sequences = torch.tensor(np.array(target_sequences), dtype=torch.long)\n",
        "\n",
        "print(input_sequences.shape, target_sequences.shape, encoded_text.shape)\n",
        "\n",
        "print(f\"1st input sequence: \\n{input_sequences[0]}\", end=\"\\n\\n\")\n",
        "print(\"Input sequence as words:\")\n",
        "print(' '.join([idx_to_word[int(i)] for i in input_sequences[0]]), end=\"\\n\\n\")\n",
        "print(\"Target sequence as words:\")\n",
        "print(' '.join([idx_to_word[int(i)] for i in target_sequences[0]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41iePTrABbgS",
        "outputId": "320ec963-53ac-47b6-b0f0-e0d806a98041"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words/vocabulary size: 14734\n",
            "torch.Size([10239, 10]) torch.Size([10239, 10]) (102390,)\n",
            "1st input sequence: \n",
            "tensor([ 2823, 14612, 13434,  8898,  1632,  4696,  4703, 13907,  8759, 13351])\n",
            "\n",
            "Input sequence as words:\n",
            "Oh yeah, this is Eminem baby, back up in that\n",
            "\n",
            "Target sequence as words:\n",
            "yeah, this is Eminem baby, back up in that motherfucking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Character level inputs and targets"
      ],
      "metadata": {
        "id": "3i-_s1FtC9rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a character-level vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "char_size = len(chars)\n",
        "print(f'Unique characters size: {char_size}')\n",
        "\n",
        "# Create mappings from characters to indices and vice versa\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "\n",
        "encoded_text = np.array([char_to_idx[char] for char in text])\n",
        "\n",
        "seq_length = 100 # Sequence length\n",
        "num_samples = len(encoded_text) // seq_length\n",
        "\n",
        "input_sequences = []\n",
        "target_sequences = []\n",
        "\n",
        "for i in range(num_samples):\n",
        "    start_idx = i * seq_length\n",
        "    end_idx = start_idx + seq_length\n",
        "    input_sequences.append(encoded_text[start_idx:end_idx])\n",
        "    target_sequences.append(encoded_text[start_idx + 1:end_idx + 1])\n",
        "\n",
        "input_sequences = torch.tensor(np.array(input_sequences), dtype=torch.long)\n",
        "target_sequences = torch.tensor(np.array(target_sequences), dtype=torch.long)\n",
        "\n",
        "print(input_sequences.shape, target_sequences.shape, encoded_text.shape)\n",
        "\n",
        "print(f\"1st input sequence: \\n{input_sequences[0]}\", end=\"\\n\\n\")\n",
        "print(\"Input sequence as words:\")\n",
        "print(''.join([idx_to_char[int(i)] for i in input_sequences[0]]), end=\"\\n\\n\")\n",
        "print(\"Target sequence as words:\")\n",
        "print(''.join([idx_to_char[int(i)] for i in target_sequences[0]]))\n"
      ],
      "metadata": {
        "id": "j4WJRwvzVJWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af290a99-fe1e-401e-8bfc-c8cd390cb14a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique characters size: 95\n",
            "torch.Size([5225, 100]) torch.Size([5225, 100]) (522527,)\n",
            "1st input sequence: \n",
            "tensor([44, 66,  1, 83, 63, 59, 66, 13,  1, 78, 66, 67, 77,  1, 67, 77,  1, 34,\n",
            "        71, 67, 72, 63, 71,  1, 60, 59, 60, 83, 13,  1, 60, 59, 61, 69,  1, 79,\n",
            "        74,  1, 67, 72,  1, 78, 66, 59, 78,  1, 71, 73, 78, 66, 63, 76, 64, 79,\n",
            "        61, 69, 67, 72, 65,  1, 59, 77, 77,  0, 44, 72, 63,  1, 78, 67, 71, 63,\n",
            "         1, 64, 73, 76,  1, 83, 73, 79, 76,  1, 71, 73, 78, 66, 63, 76,  1, 64,\n",
            "        79, 61, 69, 67, 72, 65,  1, 71, 67, 72])\n",
            "\n",
            "Input sequence as words:\n",
            "Oh yeah, this is Eminem baby, back up in that motherfucking ass\n",
            "One time for your mother fucking min\n",
            "\n",
            "Target sequence as words:\n",
            "h yeah, this is Eminem baby, back up in that motherfucking ass\n",
            "One time for your mother fucking mind\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "BB9w401aDoxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out.reshape(out.size(0) * out.size(1), out.size(2)))\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new_zeros(num_layers, batch_size, hidden_size),\n",
        "                weight.new_zeros(num_layers, batch_size, hidden_size))\n"
      ],
      "metadata": {
        "id": "uj_FPIfI-w5I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "vW6H2DzhDqxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embed_size = 512\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "model = LSTMModel(vocab_size, embed_size, hidden_size, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "9vL_n8SnAXJU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, input_sequences.size(0) - batch_size, batch_size):\n",
        "        inputs = input_sequences[i:i+batch_size]\n",
        "        targets = target_sequences[i:i+batch_size]\n",
        "\n",
        "        hidden = tuple([h.detach() for h in hidden])\n",
        "\n",
        "        # Forward\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        loss = criterion(outputs, targets.view(-1))\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / (input_sequences.size(0) // batch_size)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULnoqpqtEK5T",
        "outputId": "28b29de2-ce13-426e-9860-bc623688d2b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 7.6298\n",
            "Epoch [2/10], Loss: 7.0642\n",
            "Epoch [3/10], Loss: 6.7475\n",
            "Epoch [4/10], Loss: 6.4466\n",
            "Epoch [5/10], Loss: 6.1574\n",
            "Epoch [6/10], Loss: 5.8977\n",
            "Epoch [7/10], Loss: 5.6621\n",
            "Epoch [8/10], Loss: 5.4387\n",
            "Epoch [9/10], Loss: 5.2332\n",
            "Epoch [10/10], Loss: 5.0279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word level predicition"
      ],
      "metadata": {
        "id": "Lp5JnQ3jIouo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_word_level(model, start_str, word_to_idx, idx_to_word, num_generate=100, temperature=1.0):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the start string\n",
        "    start_tokens = start_str.split()\n",
        "    input_eval = torch.tensor([word_to_idx[word] for word in start_tokens], dtype=torch.long).unsqueeze(0)\n",
        "    hidden = model.init_hidden(1)  # Reset the hidden state\n",
        "\n",
        "    generated_text = start_tokens[:]  # Initialize with the start tokens\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_generate):\n",
        "            output, hidden = model(input_eval, hidden)\n",
        "\n",
        "            # Apply temperature and get the predicted word index\n",
        "            output = output / temperature\n",
        "            predicted_idx = torch.multinomial(torch.softmax(output[-1], dim=0), num_samples=1).item()\n",
        "\n",
        "            # Update input for the next iteration\n",
        "            input_eval = torch.tensor([[predicted_idx]], dtype=torch.long)\n",
        "\n",
        "            # Add the predicted word to the generated text\n",
        "            generated_text.append(idx_to_word[predicted_idx])\n",
        "\n",
        "    return ' '.join(generated_text)  # Join the words back into a string\n",
        "\n",
        "start_str = \"I am\"  # Example start string\n",
        "generated_text = generate_text_word_level(model, start_str, word_to_idx, idx_to_word, num_generate=50)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "AoVR5rbQIdla",
        "outputId": "2345db92-2818-4307-fa4b-b7554f70ab3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am mind, you inside shady sport Lie [Verse 3] many - till like... masterpieces It's like a small Yesterday up ([Eminem:] I'd picked my evening? for you and you I? Now are you (no queef you're trouched enough and bad there has me what I'm not up to gonna see ya\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Character level predicition\n"
      ],
      "metadata": {
        "id": "NYOsxs6FIy-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_char_level(model, start_str, char_to_idx, idx_to_char, num_generate=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    input_eval = torch.tensor([char_to_idx[c] for c in start_str], dtype=torch.long).unsqueeze(0)\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    generated_text = start_str\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_generate):\n",
        "            output, hidden = model(input_eval, hidden)\n",
        "            output = output / temperature\n",
        "            predicted_idx = torch.multinomial(torch.softmax(output[-1], dim=0), num_samples=1).item()\n",
        "            input_eval = torch.tensor([[predicted_idx]], dtype=torch.long)\n",
        "            generated_text += idx_to_char[predicted_idx]\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "start_str = \"I am \"\n",
        "generated_text = generate_text_char_level(model, start_str, char_to_idx, idx_to_char, num_generate=300)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "bXhsOZHXIsME"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}