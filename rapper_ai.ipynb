{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPxh45+K4oSfIrMgIA0GP5f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/efekaanefe/Rapper-AI/blob/main/rapper_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NFXpPuOyTlLd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('eminem-lyrics.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcSvEK1yT_0t",
        "outputId": "535baf0a-49a3-4985-aa68-1edcc13c8956"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102390"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word level inputs and targets"
      ],
      "metadata": {
        "id": "mJmUpIdEC3m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split()\n",
        "unique_words = sorted(list(set(words)))\n",
        "vocab_size = len(unique_words)\n",
        "print(f'Unique words/vocabulary size: {vocab_size}')\n",
        "\n",
        "# Create mappings from words to indices and vice versa\n",
        "word_to_idx = {word: idx for idx, word in enumerate(unique_words)}\n",
        "idx_to_word = {idx: word for idx, word in enumerate(unique_words)}\n",
        "\n",
        "# Encode the text using word-level vocabulary\n",
        "encoded_text = np.array([word_to_idx[word] for word in words])\n",
        "\n",
        "seq_length = 20  # Adjust sequence length as needed for words\n",
        "num_samples = len(encoded_text) // seq_length\n",
        "\n",
        "input_sequences = []\n",
        "target_sequences = []\n",
        "\n",
        "for i in range(num_samples):\n",
        "    start_idx = i * seq_length\n",
        "    end_idx = start_idx + seq_length\n",
        "\n",
        "    input_seq = encoded_text[start_idx:end_idx]\n",
        "    target_seq = encoded_text[start_idx + 1:end_idx + 1]\n",
        "\n",
        "    # Pad if the sequence is shorter than seq_length\n",
        "    if len(input_seq) < seq_length:\n",
        "        input_seq = np.pad(input_seq, (0, seq_length - len(input_seq)), 'constant', constant_values=0)  # Pad with zeros\n",
        "    if len(target_seq) < seq_length:\n",
        "        target_seq = np.pad(target_seq, (0, seq_length - len(target_seq)), 'constant', constant_values=0)  # Pad with zeros\n",
        "\n",
        "    input_sequences.append(input_seq)\n",
        "    target_sequences.append(target_seq)\n",
        "\n",
        "input_sequences = torch.tensor(np.array(input_sequences), dtype=torch.long)\n",
        "target_sequences = torch.tensor(np.array(target_sequences), dtype=torch.long)\n",
        "\n",
        "print(input_sequences.shape, target_sequences.shape, encoded_text.shape)\n",
        "\n",
        "print(f\"1st input sequence: \\n{input_sequences[0]}\", end=\"\\n\\n\")\n",
        "print(\"Input sequence as words:\")\n",
        "print(' '.join([idx_to_word[int(i)] for i in input_sequences[0]]), end=\"\\n\\n\")\n",
        "print(\"Target sequence as words:\")\n",
        "print(' '.join([idx_to_word[int(i)] for i in target_sequences[0]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41iePTrABbgS",
        "outputId": "47ea8a3f-70df-4dd9-82a8-e97d29f5c9f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words/vocabulary size: 14734\n",
            "torch.Size([5119, 20]) torch.Size([5119, 20]) (102390,)\n",
            "1st input sequence: \n",
            "tensor([ 2823, 14612, 13434,  8898,  1632,  4696,  4703, 13907,  8759, 13351,\n",
            "        10033,  4591,  2841, 13521,  7622, 14678, 10020,  7759,  9871, 14215])\n",
            "\n",
            "Input sequence as words:\n",
            "Oh yeah, this is Eminem baby, back up in that motherfucking ass One time for your mother fucking mind, we\n",
            "\n",
            "Target sequence as words:\n",
            "yeah, this is Eminem baby, back up in that motherfucking ass One time for your mother fucking mind, we represent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Character level inputs and targets"
      ],
      "metadata": {
        "id": "3i-_s1FtC9rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a character-level vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "char_size = len(chars)\n",
        "print(f'Unique characters size: {char_size}')\n",
        "\n",
        "# Create mappings from characters to indices and vice versa\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "\n",
        "encoded_text = np.array([char_to_idx[char] for char in text])\n",
        "\n",
        "seq_length = 100 # Sequence length\n",
        "num_samples = len(encoded_text) // seq_length\n",
        "\n",
        "input_sequences = []\n",
        "target_sequences = []\n",
        "\n",
        "for i in range(num_samples):\n",
        "    start_idx = i * seq_length\n",
        "    end_idx = start_idx + seq_length\n",
        "    input_sequences.append(encoded_text[start_idx:end_idx])\n",
        "    target_sequences.append(encoded_text[start_idx + 1:end_idx + 1])\n",
        "\n",
        "input_sequences = torch.tensor(np.array(input_sequences), dtype=torch.long)\n",
        "target_sequences = torch.tensor(np.array(target_sequences), dtype=torch.long)\n",
        "\n",
        "print(input_sequences.shape, target_sequences.shape, encoded_text.shape)\n",
        "\n",
        "print(f\"1st input sequence: \\n{input_sequences[0]}\", end=\"\\n\\n\")\n",
        "print(\"Input sequence as words:\")\n",
        "print(''.join([idx_to_char[int(i)] for i in input_sequences[0]]), end=\"\\n\\n\")\n",
        "print(\"Target sequence as words:\")\n",
        "print(''.join([idx_to_char[int(i)] for i in target_sequences[0]]))\n"
      ],
      "metadata": {
        "id": "j4WJRwvzVJWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af290a99-fe1e-401e-8bfc-c8cd390cb14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique characters size: 95\n",
            "torch.Size([5225, 100]) torch.Size([5225, 100]) (522527,)\n",
            "1st input sequence: \n",
            "tensor([44, 66,  1, 83, 63, 59, 66, 13,  1, 78, 66, 67, 77,  1, 67, 77,  1, 34,\n",
            "        71, 67, 72, 63, 71,  1, 60, 59, 60, 83, 13,  1, 60, 59, 61, 69,  1, 79,\n",
            "        74,  1, 67, 72,  1, 78, 66, 59, 78,  1, 71, 73, 78, 66, 63, 76, 64, 79,\n",
            "        61, 69, 67, 72, 65,  1, 59, 77, 77,  0, 44, 72, 63,  1, 78, 67, 71, 63,\n",
            "         1, 64, 73, 76,  1, 83, 73, 79, 76,  1, 71, 73, 78, 66, 63, 76,  1, 64,\n",
            "        79, 61, 69, 67, 72, 65,  1, 71, 67, 72])\n",
            "\n",
            "Input sequence as words:\n",
            "Oh yeah, this is Eminem baby, back up in that motherfucking ass\n",
            "One time for your mother fucking min\n",
            "\n",
            "Target sequence as words:\n",
            "h yeah, this is Eminem baby, back up in that motherfucking ass\n",
            "One time for your mother fucking mind\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "BB9w401aDoxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out.reshape(out.size(0) * out.size(1), out.size(2)))\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new_zeros(num_layers, batch_size, hidden_size),\n",
        "                weight.new_zeros(num_layers, batch_size, hidden_size))\n"
      ],
      "metadata": {
        "id": "uj_FPIfI-w5I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "vW6H2DzhDqxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embed_size = 512*4\n",
        "hidden_size = 256\n",
        "num_layers = 3\n",
        "num_epochs = 500\n",
        "learning_rate = 0.001\n",
        "batch_size = 64*4\n",
        "\n",
        "model = LSTMModel(vocab_size, embed_size, hidden_size, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "9vL_n8SnAXJU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, input_sequences.size(0) - batch_size, batch_size):\n",
        "        inputs = input_sequences[i:i+batch_size]\n",
        "        targets = target_sequences[i:i+batch_size]\n",
        "\n",
        "        hidden = tuple([h.detach() for h in hidden])\n",
        "\n",
        "        # Forward\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        loss = criterion(outputs, targets.view(-1))\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / (input_sequences.size(0) // batch_size)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULnoqpqtEK5T",
        "outputId": "0f689ac9-8867-46bd-cd82-37f6838e9faf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 8.3558\n",
            "Epoch [2/500], Loss: 7.3229\n",
            "Epoch [3/500], Loss: 7.2493\n",
            "Epoch [4/500], Loss: 7.2262\n",
            "Epoch [5/500], Loss: 7.2220\n",
            "Epoch [6/500], Loss: 7.2203\n",
            "Epoch [7/500], Loss: 7.2199\n",
            "Epoch [8/500], Loss: 7.2191\n",
            "Epoch [9/500], Loss: 7.2186\n",
            "Epoch [10/500], Loss: 7.2182\n",
            "Epoch [11/500], Loss: 7.2190\n",
            "Epoch [12/500], Loss: 7.2175\n",
            "Epoch [13/500], Loss: 7.2170\n",
            "Epoch [14/500], Loss: 7.2166\n",
            "Epoch [15/500], Loss: 7.2165\n",
            "Epoch [16/500], Loss: 7.2159\n",
            "Epoch [17/500], Loss: 7.2150\n",
            "Epoch [18/500], Loss: 7.2131\n",
            "Epoch [19/500], Loss: 7.2101\n",
            "Epoch [20/500], Loss: 7.2035\n",
            "Epoch [21/500], Loss: 7.1939\n",
            "Epoch [22/500], Loss: 7.1779\n",
            "Epoch [23/500], Loss: 7.1505\n",
            "Epoch [24/500], Loss: 7.1157\n",
            "Epoch [25/500], Loss: 7.0787\n",
            "Epoch [26/500], Loss: 7.0426\n",
            "Epoch [27/500], Loss: 7.0183\n",
            "Epoch [28/500], Loss: 6.9964\n",
            "Epoch [29/500], Loss: 6.9724\n",
            "Epoch [30/500], Loss: 6.9305\n",
            "Epoch [31/500], Loss: 6.8873\n",
            "Epoch [32/500], Loss: 6.8502\n",
            "Epoch [33/500], Loss: 6.8149\n",
            "Epoch [34/500], Loss: 6.7969\n",
            "Epoch [35/500], Loss: 6.7782\n",
            "Epoch [36/500], Loss: 6.7243\n",
            "Epoch [37/500], Loss: 6.6800\n",
            "Epoch [38/500], Loss: 6.6368\n",
            "Epoch [39/500], Loss: 6.5827\n",
            "Epoch [40/500], Loss: 6.5208\n",
            "Epoch [41/500], Loss: 6.4557\n",
            "Epoch [42/500], Loss: 6.3607\n",
            "Epoch [43/500], Loss: 6.2644\n",
            "Epoch [44/500], Loss: 6.1779\n",
            "Epoch [45/500], Loss: 6.1226\n",
            "Epoch [46/500], Loss: 6.0534\n",
            "Epoch [47/500], Loss: 5.9368\n",
            "Epoch [48/500], Loss: 5.8611\n",
            "Epoch [49/500], Loss: 5.8241\n",
            "Epoch [50/500], Loss: 5.8518\n",
            "Epoch [51/500], Loss: 5.8341\n",
            "Epoch [52/500], Loss: 5.7277\n",
            "Epoch [53/500], Loss: 5.5885\n",
            "Epoch [54/500], Loss: 5.4890\n",
            "Epoch [55/500], Loss: 5.4086\n",
            "Epoch [56/500], Loss: 5.3596\n",
            "Epoch [57/500], Loss: 5.3243\n",
            "Epoch [58/500], Loss: 5.2649\n",
            "Epoch [59/500], Loss: 5.2581\n",
            "Epoch [60/500], Loss: 5.2035\n",
            "Epoch [61/500], Loss: 5.1749\n",
            "Epoch [62/500], Loss: 5.1502\n",
            "Epoch [63/500], Loss: 5.1775\n",
            "Epoch [64/500], Loss: 5.2427\n",
            "Epoch [65/500], Loss: 5.3265\n",
            "Epoch [66/500], Loss: 5.2192\n",
            "Epoch [67/500], Loss: 5.0790\n",
            "Epoch [68/500], Loss: 4.9093\n",
            "Epoch [69/500], Loss: 4.8150\n",
            "Epoch [70/500], Loss: 4.7352\n",
            "Epoch [71/500], Loss: 4.6720\n",
            "Epoch [72/500], Loss: 4.6133\n",
            "Epoch [73/500], Loss: 4.5648\n",
            "Epoch [74/500], Loss: 4.5278\n",
            "Epoch [75/500], Loss: 4.5048\n",
            "Epoch [76/500], Loss: 4.4779\n",
            "Epoch [77/500], Loss: 4.4664\n",
            "Epoch [78/500], Loss: 4.4358\n",
            "Epoch [79/500], Loss: 4.4088\n",
            "Epoch [80/500], Loss: 4.4034\n",
            "Epoch [81/500], Loss: 4.4392\n",
            "Epoch [82/500], Loss: 4.4452\n",
            "Epoch [83/500], Loss: 4.4365\n",
            "Epoch [84/500], Loss: 4.3700\n",
            "Epoch [85/500], Loss: 4.3095\n",
            "Epoch [86/500], Loss: 4.2676\n",
            "Epoch [87/500], Loss: 4.2183\n",
            "Epoch [88/500], Loss: 4.2129\n",
            "Epoch [89/500], Loss: 4.2239\n",
            "Epoch [90/500], Loss: 4.2135\n",
            "Epoch [91/500], Loss: 4.2041\n",
            "Epoch [92/500], Loss: 4.1035\n",
            "Epoch [93/500], Loss: 4.0265\n",
            "Epoch [94/500], Loss: 3.9289\n",
            "Epoch [95/500], Loss: 3.8604\n",
            "Epoch [96/500], Loss: 3.7978\n",
            "Epoch [97/500], Loss: 3.7615\n",
            "Epoch [98/500], Loss: 3.7275\n",
            "Epoch [99/500], Loss: 3.7029\n",
            "Epoch [100/500], Loss: 3.6955\n",
            "Epoch [101/500], Loss: 3.6940\n",
            "Epoch [102/500], Loss: 3.6836\n",
            "Epoch [103/500], Loss: 3.6633\n",
            "Epoch [104/500], Loss: 3.6795\n",
            "Epoch [105/500], Loss: 3.6739\n",
            "Epoch [106/500], Loss: 3.6280\n",
            "Epoch [107/500], Loss: 3.6072\n",
            "Epoch [108/500], Loss: 3.5182\n",
            "Epoch [109/500], Loss: 3.4633\n",
            "Epoch [110/500], Loss: 3.3989\n",
            "Epoch [111/500], Loss: 3.3430\n",
            "Epoch [112/500], Loss: 3.2931\n",
            "Epoch [113/500], Loss: 3.2480\n",
            "Epoch [114/500], Loss: 3.2287\n",
            "Epoch [115/500], Loss: 3.2248\n",
            "Epoch [116/500], Loss: 3.2113\n",
            "Epoch [117/500], Loss: 3.2014\n",
            "Epoch [118/500], Loss: 3.2039\n",
            "Epoch [119/500], Loss: 3.1549\n",
            "Epoch [120/500], Loss: 3.1409\n",
            "Epoch [121/500], Loss: 3.0916\n",
            "Epoch [122/500], Loss: 3.0849\n",
            "Epoch [123/500], Loss: 3.0953\n",
            "Epoch [124/500], Loss: 3.0911\n",
            "Epoch [125/500], Loss: 3.0775\n",
            "Epoch [126/500], Loss: 3.0932\n",
            "Epoch [127/500], Loss: 3.0809\n",
            "Epoch [128/500], Loss: 3.1182\n",
            "Epoch [129/500], Loss: 3.0587\n",
            "Epoch [130/500], Loss: 3.0282\n",
            "Epoch [131/500], Loss: 2.9734\n",
            "Epoch [132/500], Loss: 2.9488\n",
            "Epoch [133/500], Loss: 2.8522\n",
            "Epoch [134/500], Loss: 2.7707\n",
            "Epoch [135/500], Loss: 2.7323\n",
            "Epoch [136/500], Loss: 2.6913\n",
            "Epoch [137/500], Loss: 2.6654\n",
            "Epoch [138/500], Loss: 2.6701\n",
            "Epoch [139/500], Loss: 2.6500\n",
            "Epoch [140/500], Loss: 2.6396\n",
            "Epoch [141/500], Loss: 2.6089\n",
            "Epoch [142/500], Loss: 2.6340\n",
            "Epoch [143/500], Loss: 2.6605\n",
            "Epoch [144/500], Loss: 2.7319\n",
            "Epoch [145/500], Loss: 2.7522\n",
            "Epoch [146/500], Loss: 2.7190\n",
            "Epoch [147/500], Loss: 2.6274\n",
            "Epoch [148/500], Loss: 2.5660\n",
            "Epoch [149/500], Loss: 2.4811\n",
            "Epoch [150/500], Loss: 2.4449\n",
            "Epoch [151/500], Loss: 2.3671\n",
            "Epoch [152/500], Loss: 2.3117\n",
            "Epoch [153/500], Loss: 2.2746\n",
            "Epoch [154/500], Loss: 2.2514\n",
            "Epoch [155/500], Loss: 2.2219\n",
            "Epoch [156/500], Loss: 2.2235\n",
            "Epoch [157/500], Loss: 2.1744\n",
            "Epoch [158/500], Loss: 2.1584\n",
            "Epoch [159/500], Loss: 2.1347\n",
            "Epoch [160/500], Loss: 2.1277\n",
            "Epoch [161/500], Loss: 2.1152\n",
            "Epoch [162/500], Loss: 2.1020\n",
            "Epoch [163/500], Loss: 2.0778\n",
            "Epoch [164/500], Loss: 2.0814\n",
            "Epoch [165/500], Loss: 2.0691\n",
            "Epoch [166/500], Loss: 2.0772\n",
            "Epoch [167/500], Loss: 2.0718\n",
            "Epoch [168/500], Loss: 2.0740\n",
            "Epoch [169/500], Loss: 2.0210\n",
            "Epoch [170/500], Loss: 2.0014\n",
            "Epoch [171/500], Loss: 1.9599\n",
            "Epoch [172/500], Loss: 1.9572\n",
            "Epoch [173/500], Loss: 1.9565\n",
            "Epoch [174/500], Loss: 1.9519\n",
            "Epoch [175/500], Loss: 1.9744\n",
            "Epoch [176/500], Loss: 1.9877\n",
            "Epoch [177/500], Loss: 1.9785\n",
            "Epoch [178/500], Loss: 2.0109\n",
            "Epoch [179/500], Loss: 1.9996\n",
            "Epoch [180/500], Loss: 1.9845\n",
            "Epoch [181/500], Loss: 1.9659\n",
            "Epoch [182/500], Loss: 1.9425\n",
            "Epoch [183/500], Loss: 1.8478\n",
            "Epoch [184/500], Loss: 1.7774\n",
            "Epoch [185/500], Loss: 1.7344\n",
            "Epoch [186/500], Loss: 1.6697\n",
            "Epoch [187/500], Loss: 1.6561\n",
            "Epoch [188/500], Loss: 1.6231\n",
            "Epoch [189/500], Loss: 1.5867\n",
            "Epoch [190/500], Loss: 1.5767\n",
            "Epoch [191/500], Loss: 1.5446\n",
            "Epoch [192/500], Loss: 1.5328\n",
            "Epoch [193/500], Loss: 1.5274\n",
            "Epoch [194/500], Loss: 1.5149\n",
            "Epoch [195/500], Loss: 1.5131\n",
            "Epoch [196/500], Loss: 1.5270\n",
            "Epoch [197/500], Loss: 1.5132\n",
            "Epoch [198/500], Loss: 1.5335\n",
            "Epoch [199/500], Loss: 1.5450\n",
            "Epoch [200/500], Loss: 1.5018\n",
            "Epoch [201/500], Loss: 1.4833\n",
            "Epoch [202/500], Loss: 1.4874\n",
            "Epoch [203/500], Loss: 1.4443\n",
            "Epoch [204/500], Loss: 1.4252\n",
            "Epoch [205/500], Loss: 1.4023\n",
            "Epoch [206/500], Loss: 1.3794\n",
            "Epoch [207/500], Loss: 1.3701\n",
            "Epoch [208/500], Loss: 1.3389\n",
            "Epoch [209/500], Loss: 1.3399\n",
            "Epoch [210/500], Loss: 1.3186\n",
            "Epoch [211/500], Loss: 1.2734\n",
            "Epoch [212/500], Loss: 1.2724\n",
            "Epoch [213/500], Loss: 1.2358\n",
            "Epoch [214/500], Loss: 1.2233\n",
            "Epoch [215/500], Loss: 1.1920\n",
            "Epoch [216/500], Loss: 1.1491\n",
            "Epoch [217/500], Loss: 1.1311\n",
            "Epoch [218/500], Loss: 1.1118\n",
            "Epoch [219/500], Loss: 1.0973\n",
            "Epoch [220/500], Loss: 1.0957\n",
            "Epoch [221/500], Loss: 1.0875\n",
            "Epoch [222/500], Loss: 1.0806\n",
            "Epoch [223/500], Loss: 1.0875\n",
            "Epoch [224/500], Loss: 1.0925\n",
            "Epoch [225/500], Loss: 1.0874\n",
            "Epoch [226/500], Loss: 1.0954\n",
            "Epoch [227/500], Loss: 1.1056\n",
            "Epoch [228/500], Loss: 1.1147\n",
            "Epoch [229/500], Loss: 1.1118\n",
            "Epoch [230/500], Loss: 1.1235\n",
            "Epoch [231/500], Loss: 1.1110\n",
            "Epoch [232/500], Loss: 1.1115\n",
            "Epoch [233/500], Loss: 1.1133\n",
            "Epoch [234/500], Loss: 1.0992\n",
            "Epoch [235/500], Loss: 1.0917\n",
            "Epoch [236/500], Loss: 1.1409\n",
            "Epoch [237/500], Loss: 1.1254\n",
            "Epoch [238/500], Loss: 1.1123\n",
            "Epoch [239/500], Loss: 1.1182\n",
            "Epoch [240/500], Loss: 1.0470\n",
            "Epoch [241/500], Loss: 1.0243\n",
            "Epoch [242/500], Loss: 0.9712\n",
            "Epoch [243/500], Loss: 0.9403\n",
            "Epoch [244/500], Loss: 0.9032\n",
            "Epoch [245/500], Loss: 0.8790\n",
            "Epoch [246/500], Loss: 0.8489\n",
            "Epoch [247/500], Loss: 0.8354\n",
            "Epoch [248/500], Loss: 0.8219\n",
            "Epoch [249/500], Loss: 0.8118\n",
            "Epoch [250/500], Loss: 0.7940\n",
            "Epoch [251/500], Loss: 0.7788\n",
            "Epoch [252/500], Loss: 0.7704\n",
            "Epoch [253/500], Loss: 0.7629\n",
            "Epoch [254/500], Loss: 0.7569\n",
            "Epoch [255/500], Loss: 0.7604\n",
            "Epoch [256/500], Loss: 0.7564\n",
            "Epoch [257/500], Loss: 0.7615\n",
            "Epoch [258/500], Loss: 0.7644\n",
            "Epoch [259/500], Loss: 0.7658\n",
            "Epoch [260/500], Loss: 0.7649\n",
            "Epoch [261/500], Loss: 0.7679\n",
            "Epoch [262/500], Loss: 0.7768\n",
            "Epoch [263/500], Loss: 0.7823\n",
            "Epoch [264/500], Loss: 0.7763\n",
            "Epoch [265/500], Loss: 0.7918\n",
            "Epoch [266/500], Loss: 0.8009\n",
            "Epoch [267/500], Loss: 0.7760\n",
            "Epoch [268/500], Loss: 0.7793\n",
            "Epoch [269/500], Loss: 0.7970\n",
            "Epoch [270/500], Loss: 0.7939\n",
            "Epoch [271/500], Loss: 0.8050\n",
            "Epoch [272/500], Loss: 0.8468\n",
            "Epoch [273/500], Loss: 0.8532\n",
            "Epoch [274/500], Loss: 0.8214\n",
            "Epoch [275/500], Loss: 0.8270\n",
            "Epoch [276/500], Loss: 0.7756\n",
            "Epoch [277/500], Loss: 0.7406\n",
            "Epoch [278/500], Loss: 0.7136\n",
            "Epoch [279/500], Loss: 0.6646\n",
            "Epoch [280/500], Loss: 0.6367\n",
            "Epoch [281/500], Loss: 0.6040\n",
            "Epoch [282/500], Loss: 0.5835\n",
            "Epoch [283/500], Loss: 0.5703\n",
            "Epoch [284/500], Loss: 0.5537\n",
            "Epoch [285/500], Loss: 0.5430\n",
            "Epoch [286/500], Loss: 0.5375\n",
            "Epoch [287/500], Loss: 0.5410\n",
            "Epoch [288/500], Loss: 0.5360\n",
            "Epoch [289/500], Loss: 0.5352\n",
            "Epoch [290/500], Loss: 0.5431\n",
            "Epoch [291/500], Loss: 0.5537\n",
            "Epoch [292/500], Loss: 0.5509\n",
            "Epoch [293/500], Loss: 0.5593\n",
            "Epoch [294/500], Loss: 0.5801\n",
            "Epoch [295/500], Loss: 0.5966\n",
            "Epoch [296/500], Loss: 0.5875\n",
            "Epoch [297/500], Loss: 0.6179\n",
            "Epoch [298/500], Loss: 0.6548\n",
            "Epoch [299/500], Loss: 0.6612\n",
            "Epoch [300/500], Loss: 0.6622\n",
            "Epoch [301/500], Loss: 0.6819\n",
            "Epoch [302/500], Loss: 0.6844\n",
            "Epoch [303/500], Loss: 0.6644\n",
            "Epoch [304/500], Loss: 0.6793\n",
            "Epoch [305/500], Loss: 0.6717\n",
            "Epoch [306/500], Loss: 0.6426\n",
            "Epoch [307/500], Loss: 0.6263\n",
            "Epoch [308/500], Loss: 0.6098\n",
            "Epoch [309/500], Loss: 0.5785\n",
            "Epoch [310/500], Loss: 0.5308\n",
            "Epoch [311/500], Loss: 0.4963\n",
            "Epoch [312/500], Loss: 0.4548\n",
            "Epoch [313/500], Loss: 0.4296\n",
            "Epoch [314/500], Loss: 0.4067\n",
            "Epoch [315/500], Loss: 0.3928\n",
            "Epoch [316/500], Loss: 0.3812\n",
            "Epoch [317/500], Loss: 0.3735\n",
            "Epoch [318/500], Loss: 0.3671\n",
            "Epoch [319/500], Loss: 0.3635\n",
            "Epoch [320/500], Loss: 0.3559\n",
            "Epoch [321/500], Loss: 0.3528\n",
            "Epoch [322/500], Loss: 0.3475\n",
            "Epoch [323/500], Loss: 0.3444\n",
            "Epoch [324/500], Loss: 0.3411\n",
            "Epoch [325/500], Loss: 0.3389\n",
            "Epoch [326/500], Loss: 0.3372\n",
            "Epoch [327/500], Loss: 0.3316\n",
            "Epoch [328/500], Loss: 0.3301\n",
            "Epoch [329/500], Loss: 0.3269\n",
            "Epoch [330/500], Loss: 0.3269\n",
            "Epoch [331/500], Loss: 0.3263\n",
            "Epoch [332/500], Loss: 0.3253\n",
            "Epoch [333/500], Loss: 0.3245\n",
            "Epoch [334/500], Loss: 0.3249\n",
            "Epoch [335/500], Loss: 0.3240\n",
            "Epoch [336/500], Loss: 0.3278\n",
            "Epoch [337/500], Loss: 0.3336\n",
            "Epoch [338/500], Loss: 0.3424\n",
            "Epoch [339/500], Loss: 0.3545\n",
            "Epoch [340/500], Loss: 0.3650\n",
            "Epoch [341/500], Loss: 0.3830\n",
            "Epoch [342/500], Loss: 0.3920\n",
            "Epoch [343/500], Loss: 0.3966\n",
            "Epoch [344/500], Loss: 0.4049\n",
            "Epoch [345/500], Loss: 0.4222\n",
            "Epoch [346/500], Loss: 0.4491\n",
            "Epoch [347/500], Loss: 0.4861\n",
            "Epoch [348/500], Loss: 0.5212\n",
            "Epoch [349/500], Loss: 0.5527\n",
            "Epoch [350/500], Loss: 0.6053\n",
            "Epoch [351/500], Loss: 0.6327\n",
            "Epoch [352/500], Loss: 0.6960\n",
            "Epoch [353/500], Loss: 0.7344\n",
            "Epoch [354/500], Loss: 0.8340\n",
            "Epoch [355/500], Loss: 0.8487\n",
            "Epoch [356/500], Loss: 0.8648\n",
            "Epoch [357/500], Loss: 0.8787\n",
            "Epoch [358/500], Loss: 0.8104\n",
            "Epoch [359/500], Loss: 0.8393\n",
            "Epoch [360/500], Loss: 0.8765\n",
            "Epoch [361/500], Loss: 0.7878\n",
            "Epoch [362/500], Loss: 0.7075\n",
            "Epoch [363/500], Loss: 0.5778\n",
            "Epoch [364/500], Loss: 0.4511\n",
            "Epoch [365/500], Loss: 0.3612\n",
            "Epoch [366/500], Loss: 0.3131\n",
            "Epoch [367/500], Loss: 0.2843\n",
            "Epoch [368/500], Loss: 0.2686\n",
            "Epoch [369/500], Loss: 0.2591\n",
            "Epoch [370/500], Loss: 0.2519\n",
            "Epoch [371/500], Loss: 0.2459\n",
            "Epoch [372/500], Loss: 0.2408\n",
            "Epoch [373/500], Loss: 0.2361\n",
            "Epoch [374/500], Loss: 0.2319\n",
            "Epoch [375/500], Loss: 0.2281\n",
            "Epoch [376/500], Loss: 0.2243\n",
            "Epoch [377/500], Loss: 0.2209\n",
            "Epoch [378/500], Loss: 0.2176\n",
            "Epoch [379/500], Loss: 0.2145\n",
            "Epoch [380/500], Loss: 0.2113\n",
            "Epoch [381/500], Loss: 0.2082\n",
            "Epoch [382/500], Loss: 0.2053\n",
            "Epoch [383/500], Loss: 0.2027\n",
            "Epoch [384/500], Loss: 0.2000\n",
            "Epoch [385/500], Loss: 0.1978\n",
            "Epoch [386/500], Loss: 0.1952\n",
            "Epoch [387/500], Loss: 0.1930\n",
            "Epoch [388/500], Loss: 0.1915\n",
            "Epoch [389/500], Loss: 0.1920\n",
            "Epoch [390/500], Loss: 0.1874\n",
            "Epoch [391/500], Loss: 0.1841\n",
            "Epoch [392/500], Loss: 0.1817\n",
            "Epoch [393/500], Loss: 0.1796\n",
            "Epoch [394/500], Loss: 0.1774\n",
            "Epoch [395/500], Loss: 0.1751\n",
            "Epoch [396/500], Loss: 0.1728\n",
            "Epoch [397/500], Loss: 0.1712\n",
            "Epoch [398/500], Loss: 0.1690\n",
            "Epoch [399/500], Loss: 0.1686\n",
            "Epoch [400/500], Loss: 0.1682\n",
            "Epoch [401/500], Loss: 0.1651\n",
            "Epoch [402/500], Loss: 0.1625\n",
            "Epoch [403/500], Loss: 0.1614\n",
            "Epoch [404/500], Loss: 0.1594\n",
            "Epoch [405/500], Loss: 0.1590\n",
            "Epoch [406/500], Loss: 0.1574\n",
            "Epoch [407/500], Loss: 0.1555\n",
            "Epoch [408/500], Loss: 0.1549\n",
            "Epoch [409/500], Loss: 0.1531\n",
            "Epoch [410/500], Loss: 0.1521\n",
            "Epoch [411/500], Loss: 0.1506\n",
            "Epoch [412/500], Loss: 0.1493\n",
            "Epoch [413/500], Loss: 0.1481\n",
            "Epoch [414/500], Loss: 0.1478\n",
            "Epoch [415/500], Loss: 0.1452\n",
            "Epoch [416/500], Loss: 0.1444\n",
            "Epoch [417/500], Loss: 0.1442\n",
            "Epoch [418/500], Loss: 0.1440\n",
            "Epoch [419/500], Loss: 0.1443\n",
            "Epoch [420/500], Loss: 0.1429\n",
            "Epoch [421/500], Loss: 0.1426\n",
            "Epoch [422/500], Loss: 0.1419\n",
            "Epoch [423/500], Loss: 0.1409\n",
            "Epoch [424/500], Loss: 0.1414\n",
            "Epoch [425/500], Loss: 0.1422\n",
            "Epoch [426/500], Loss: 0.1458\n",
            "Epoch [427/500], Loss: 0.1470\n",
            "Epoch [428/500], Loss: 0.1494\n",
            "Epoch [429/500], Loss: 0.1529\n",
            "Epoch [430/500], Loss: 0.1582\n",
            "Epoch [431/500], Loss: 0.1626\n",
            "Epoch [432/500], Loss: 0.1670\n",
            "Epoch [433/500], Loss: 0.1754\n",
            "Epoch [434/500], Loss: 0.1818\n",
            "Epoch [435/500], Loss: 0.1881\n",
            "Epoch [436/500], Loss: 0.1927\n",
            "Epoch [437/500], Loss: 0.1985\n",
            "Epoch [438/500], Loss: 0.2033\n",
            "Epoch [439/500], Loss: 0.2023\n",
            "Epoch [440/500], Loss: 0.1961\n",
            "Epoch [441/500], Loss: 0.1905\n",
            "Epoch [442/500], Loss: 0.1781\n",
            "Epoch [443/500], Loss: 0.1659\n",
            "Epoch [444/500], Loss: 0.1559\n",
            "Epoch [445/500], Loss: 0.1480\n",
            "Epoch [446/500], Loss: 0.1384\n",
            "Epoch [447/500], Loss: 0.1314\n",
            "Epoch [448/500], Loss: 0.1281\n",
            "Epoch [449/500], Loss: 0.1217\n",
            "Epoch [450/500], Loss: 0.1208\n",
            "Epoch [451/500], Loss: 0.1147\n",
            "Epoch [452/500], Loss: 0.1101\n",
            "Epoch [453/500], Loss: 0.1080\n",
            "Epoch [454/500], Loss: 0.1071\n",
            "Epoch [455/500], Loss: 0.1063\n",
            "Epoch [456/500], Loss: 0.1035\n",
            "Epoch [457/500], Loss: 0.1033\n",
            "Epoch [458/500], Loss: 0.1049\n",
            "Epoch [459/500], Loss: 0.1074\n",
            "Epoch [460/500], Loss: 0.1093\n",
            "Epoch [461/500], Loss: 0.1134\n",
            "Epoch [462/500], Loss: 0.1209\n",
            "Epoch [463/500], Loss: 0.1294\n",
            "Epoch [464/500], Loss: 0.1422\n",
            "Epoch [465/500], Loss: 0.1651\n",
            "Epoch [466/500], Loss: 0.2042\n",
            "Epoch [467/500], Loss: 0.2694\n",
            "Epoch [468/500], Loss: 0.3536\n",
            "Epoch [469/500], Loss: 0.4547\n",
            "Epoch [470/500], Loss: 0.5625\n",
            "Epoch [471/500], Loss: 0.6417\n",
            "Epoch [472/500], Loss: 0.6829\n",
            "Epoch [473/500], Loss: 0.6900\n",
            "Epoch [474/500], Loss: 0.7269\n",
            "Epoch [475/500], Loss: 0.7670\n",
            "Epoch [476/500], Loss: 0.7656\n",
            "Epoch [477/500], Loss: 0.7379\n",
            "Epoch [478/500], Loss: 0.7292\n",
            "Epoch [479/500], Loss: 0.6847\n",
            "Epoch [480/500], Loss: 0.5710\n",
            "Epoch [481/500], Loss: 0.4408\n",
            "Epoch [482/500], Loss: 0.3244\n",
            "Epoch [483/500], Loss: 0.2271\n",
            "Epoch [484/500], Loss: 0.1697\n",
            "Epoch [485/500], Loss: 0.1354\n",
            "Epoch [486/500], Loss: 0.1169\n",
            "Epoch [487/500], Loss: 0.1085\n",
            "Epoch [488/500], Loss: 0.1025\n",
            "Epoch [489/500], Loss: 0.0983\n",
            "Epoch [490/500], Loss: 0.0949\n",
            "Epoch [491/500], Loss: 0.0921\n",
            "Epoch [492/500], Loss: 0.0897\n",
            "Epoch [493/500], Loss: 0.0875\n",
            "Epoch [494/500], Loss: 0.0855\n",
            "Epoch [495/500], Loss: 0.0837\n",
            "Epoch [496/500], Loss: 0.0819\n",
            "Epoch [497/500], Loss: 0.0802\n",
            "Epoch [498/500], Loss: 0.0785\n",
            "Epoch [499/500], Loss: 0.0771\n",
            "Epoch [500/500], Loss: 0.0757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word level predicition"
      ],
      "metadata": {
        "id": "Lp5JnQ3jIouo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "# load the model\n",
        "model = LSTMModel(vocab_size, embed_size, hidden_size, num_layers)\n",
        "model.load_state_dict(torch.load('model.pth'))\n"
      ],
      "metadata": {
        "id": "wEQUs3uzcWMP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "def generate_text_word_level(model, start_str, word_to_idx, idx_to_word, num_generate=50, temperature=1.0):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the start string\n",
        "    start_tokens = start_str.split()\n",
        "    input_eval = torch.tensor([word_to_idx[word] for word in start_tokens], dtype=torch.long).unsqueeze(0)\n",
        "    hidden = model.init_hidden(1)  # Reset the hidden state\n",
        "\n",
        "    generated_text = start_tokens[:]  # Initialize with the start tokens\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_generate):\n",
        "            output, hidden = model(input_eval, hidden)\n",
        "\n",
        "            # Apply temperature and get the predicted word index\n",
        "            output = output / temperature\n",
        "            predicted_idx = torch.multinomial(torch.softmax(output[-1], dim=0), num_samples=1).item()\n",
        "\n",
        "            # Update input for the next iteration\n",
        "            input_eval = torch.tensor([[predicted_idx]], dtype=torch.long)\n",
        "\n",
        "            # Add the predicted word to the generated text\n",
        "            generated_text.append(idx_to_word[predicted_idx])\n",
        "\n",
        "    return ' '.join(generated_text)  # Join the words back into a string\n",
        "\n",
        "start_str = \"Oh yeah, this is Eminem baby\"\n",
        "generated_text = generate_text_word_level(model, start_str, word_to_idx, idx_to_word, num_generate=50)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "AoVR5rbQIdla",
        "outputId": "61651632-0119-4473-ecd0-437e88a2d19e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh yeah, this is Eminem baby 'Cause there daddy's shot that shit up around I'll take no picture now his hands But your hands in the rock and sit is back to the damn bit down, it back for my ass, if it's over, we know on this Earth And shoot no man over steam Sit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Character level predicition\n"
      ],
      "metadata": {
        "id": "NYOsxs6FIy-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_char_level(model, start_str, char_to_idx, idx_to_char, num_generate=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    input_eval = torch.tensor([char_to_idx[c] for c in start_str], dtype=torch.long).unsqueeze(0)\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    generated_text = start_str\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_generate):\n",
        "            output, hidden = model(input_eval, hidden)\n",
        "            output = output / temperature\n",
        "            predicted_idx = torch.multinomial(torch.softmax(output[-1], dim=0), num_samples=1).item()\n",
        "            input_eval = torch.tensor([[predicted_idx]], dtype=torch.long)\n",
        "            generated_text += idx_to_char[predicted_idx]\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "start_str = \"I am \"\n",
        "generated_text = generate_text_char_level(model, start_str, char_to_idx, idx_to_char, num_generate=300)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "bXhsOZHXIsME"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}